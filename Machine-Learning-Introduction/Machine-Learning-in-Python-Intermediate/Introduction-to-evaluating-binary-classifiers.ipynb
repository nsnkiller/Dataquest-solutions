{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions = pd.read_csv(\"admissions.csv\")\n",
    "model = LogisticRegression()\n",
    "model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    507\n",
      "1    137\n",
      "Name: predicted_label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.177277</td>\n",
       "      <td>594.102992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.412655</td>\n",
       "      <td>631.528607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.728097</td>\n",
       "      <td>553.714399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.093559</td>\n",
       "      <td>551.089985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.141923</td>\n",
       "      <td>537.184894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit       gpa         gre  predicted_label\n",
       "0      0  3.177277  594.102992                0\n",
       "1      0  3.412655  631.528607                0\n",
       "2      0  2.728097  553.714399                0\n",
       "3      0  3.093559  551.089985                0\n",
       "4      0  3.141923  537.184894                0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = model.predict(admissions[['gpa']])\n",
    "admissions['predicted_label'] = labels\n",
    "\n",
    "print(admissions['predicted_label'].value_counts())\n",
    "\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy helps us answer the question:\n",
    "\n",
    "* **<font size=4>What fraction of the predictions were correct (actual label matched predicted label)?</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Accuracy = \\dfrac{\\text{# of Correctly Predicted}}{\\text{# of Observations}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {'admit': 'actual_label'}\n",
    "admissions.rename(mapper, axis=1, inplace=1)\n",
    "matches = admissions['actual_label'] == admissions['predicted_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "      <td>3.220292</td>\n",
       "      <td>680.028661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>3.036726</td>\n",
       "      <td>562.163269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>3.055598</td>\n",
       "      <td>686.667456</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>2.957306</td>\n",
       "      <td>620.739871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>1</td>\n",
       "      <td>3.532418</td>\n",
       "      <td>677.019051</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual_label       gpa         gre  predicted_label\n",
       "116             0  3.220292  680.028661                0\n",
       "89              0  3.036726  562.163269                0\n",
       "200             0  3.055598  686.667456                0\n",
       "31              0  2.957306  620.739871                0\n",
       "625             1  3.532418  677.019051                1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = admissions[matches]\n",
    "accuracy = correct_predictions.shape[0]/admissions.shape[0]\n",
    "correct_predictions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6847826086956522"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<table class=\"table-bordered\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th >Prediction</th>\n",
    "<th><center>Observation</center></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<thead>\n",
    "<tr>\n",
    "<th></th>\n",
    "<th>Admitted (1) </th>\n",
    "<th>Rejected (0) </th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<th>Admitted (1) </th>\n",
    "<td>True Positive (TP)</td>\n",
    "<td>False Positive (FP)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>Rejected (0) </th>\n",
    "<td>False Negative (FN)</td>\n",
    "<td>True Negative (TN)</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define these outcomes as:\n",
    "\n",
    "* True Positive - The model correctly predicted that the student would be admitted.\n",
    "\n",
    "    - Said another way, the model predicted that the label would be `Positive`, and that ended up being `True`.\n",
    "    - In our case, Positive refers to being admitted and maps to the label 1 in the dataset.\n",
    "    - For this dataset, a true positive is whenever predicted_label is 1 and actual_label is 1.\n",
    "\n",
    "\n",
    "* True Negative - The model correctly predicted that the student would be rejected.\n",
    "\n",
    "    - Said another way, the model predicted that the label would be `Negative`, and that ended up being `True`.\n",
    "    - In our case, Negative refers to being rejected and maps to the label 0 in the dataset.\n",
    "    - For this dataset, a true negative is whenever predicted_label is 0 and actual_label is 0.\n",
    "\n",
    "\n",
    "* False Positive - The model incorrectly predicted that the student would be admitted even though the student was actually rejected.\n",
    "\n",
    "    - Said another way, the model predicted that the label would be `Positive`, but that was `False` (the actual label was False).\n",
    "    - For this dataset, a false positive is whenever predicted_label is 1 but the actual_label is 0.\n",
    "\n",
    "\n",
    "* False Negative - The model incorrectly predicted that the student would be rejected even though the student was actually admitted.\n",
    "\n",
    "    - Said another way, the model predicted that the label would be `Negative`, but that was `False` (the actual value was True).\n",
    "    - For this dataset, a false negative is whenever predicted_label is 0 but the actual_label is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = admissions[(admissions['actual_label'] == admissions['predicted_label'])&(admissions['predicted_label']==1)].shape[0]\n",
    "\n",
    "true_negatives = admissions[(admissions['actual_label'] == admissions['predicted_label'])&(admissions['predicted_label']==0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "print(true_positives)\n",
    "print(true_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a few measures that are much more insightful than simple accuracy. Let's start with **sensitivity**:\n",
    "\n",
    "* Sensitivity or True Positive Rate - The proportion of applicants that were correctly admitted:\n",
    "\n",
    "**$TPR=\\dfrac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all of the students that should have been admitted (True Positives + False Negatives), what fraction did the model correctly admit (True Positives)? More generally, this measure helps us answer the question:\n",
    "\n",
    "* **How effective is this model at identifying positive outcomes?**\n",
    "In our case, the positive outcome (label of `1`) is admitting a student. If the True Positive Rate is low, it means that the model isn't effective at catching positive cases. For certain problems, high sensitivity is incredibly important. If we're building a model to predict which patients have cancer, every patient that is missed by the model could mean a loss of life. We want a **highly sensitive** model that is able to \"catch\" all of the positive cases (in this case, the positive case is a patient with cancer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36475409836065575"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives = admissions[(admissions['actual_label'] != admissions['predicted_label']) & (admissions['predicted_label'] == 0)].shape[0]\n",
    "\n",
    "sensitivity = true_positives/(true_positives + false_negatives)\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TNR=\\dfrac{\\text{True Negatives}}{\\text{False Positives} + \\text{True Negatives}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps us answer the question:\n",
    "\n",
    "* **How effective is this model at identifying negative outcomes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = true_negatives/(admissions.shape[0] - admissions['actual_label'].sum())\n",
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives = admissions[(admissions['actual_label']==0)&(admissions['predicted_label']==1)].shape[0]\n",
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
