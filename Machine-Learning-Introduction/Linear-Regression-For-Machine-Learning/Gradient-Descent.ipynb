{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mission and the next, we'll discuss the two most common ways for finding the optimal parameter values for a linear regression model. Each combination of unique parameter values forms a unique linear regression model, and the process of finding these optimal values is known as **<font color=blue>model fitting</font>**. In both approaches to model fitting, we'll aim to minimize the following function:\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} ({\\hat{y_i} - y_i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{SalePrice} = a_1 * Gr Liv Area$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Jupyter](./single_var_operation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Variable Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an overview of the gradient descent algorithm for a single parameter linear regression model:\n",
    "\n",
    "* select initial values for the parameter: $a1$\n",
    "* repeat until convergence (usually implemented with a max number of iterations):\n",
    "    - calculate the error (MSE) of model that uses current parameter value: $MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)} ) ^2$\n",
    "    - calculate the derivative of the error (MSE) at the current parameter value: $\\frac{d}{da_1} MSE(a_1)$\n",
    "    - update the parameter value by subtracting the derivative times a constant ($\\alpha$, called the learning rate): $a_1 := a_1 - \\alpha \\frac{d}{da_1} MSE(a_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting an appropriate `initial parameter` and `learning rate` will reduce the number of iterations required to converge, and is part of hyperparameter optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Of The Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical optimization, a function that we optimize through minimization is known as a **cost function** or sometimes as the [loss function](https://en.wikipedia.org/wiki/Loss_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(a1, xi_list, yi_list):\n",
    "    # Modify this function.\n",
    "    n = len(xi_list)\n",
    "    cost_err = 0\n",
    "    for i in range(n):\n",
    "        cost_err += (a1*xi_list[i] - yi_list[i])*xi_list[i]\n",
    "    cost_err = 2*cost_err/n\n",
    "    return cost_err\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):\n",
    "    a1_list = [a1_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        deriv = derivative(a1, xi_list, yi_list)\n",
    "        a1_new = a1 - alpha*deriv\n",
    "        a1_list.append(a1_new)\n",
    "    return(a1_list)\n",
    "\n",
    "# Uncomment when ready.\n",
    "param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)\n",
    "final_param = param_iterations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120.14219147202738"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[150,\n",
       " 106.24258269493151,\n",
       " 126.61281661731272,\n",
       " 117.12993450021699,\n",
       " 121.54446668425497,\n",
       " 119.48938531096931,\n",
       " 120.44607998998796,\n",
       " 120.00071333893449,\n",
       " 120.20804328256295,\n",
       " 120.11152571569237,\n",
       " 120.15645719327628,\n",
       " 120.13554040327286,\n",
       " 120.1452777216869,\n",
       " 120.14074474268385,\n",
       " 120.14285496418101,\n",
       " 120.14187260031741,\n",
       " 120.14232991665213,\n",
       " 120.142117023815,\n",
       " 120.14221613105579,\n",
       " 120.14216999401657,\n",
       " 120.14219147202738]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Multi Parameter Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SalePrice = a_1 * Gr Liv Area + a_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below image, we've generated a 3D scatter plot with:\n",
    "* $a_0$ on the x-axis\n",
    "* $a_1$ on the y-axis\n",
    "* $MSE$ on the z-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Jupyter](./surface_plot.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Of The Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a1_derivative(a0, a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += xi_list[i]*(a0 + a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def a0_derivative(a0, a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += (a0 + a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):\n",
    "    a1_list = [a1_initial]\n",
    "    a0_list = [a0_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        a0 = a0_list[i]\n",
    "        \n",
    "        a1_deriv = a1_derivative(a0, a1, xi_list, yi_list)\n",
    "        a0_deriv = a0_derivative(a0, a1, xi_list, yi_list)\n",
    "        \n",
    "        a1_new = a1 - alpha*a1_deriv\n",
    "        a0_new = a0 - alpha*a0_deriv\n",
    "        \n",
    "        a1_list.append(a1_new)\n",
    "        a0_list.append(a0_new)\n",
    "    return(a0_list, a1_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when ready.\n",
    "a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000,\n",
       " 999.9729797812329,\n",
       " 999.985903701066,\n",
       " 999.980232547139,\n",
       " 999.9832179015052,\n",
       " 999.9821734177915,\n",
       " 999.983004932363,\n",
       " 999.9829631191217,\n",
       " 999.9833278635107,\n",
       " 999.98350334434,\n",
       " 999.9837669324418,\n",
       " 999.9839895042135,\n",
       " 999.9842311701743,\n",
       " 999.9844639472566,\n",
       " 999.9847008623329,\n",
       " 999.9849358510428,\n",
       " 999.9851717365096,\n",
       " 999.9854072044933,\n",
       " 999.985642866808,\n",
       " 999.9858784386378,\n",
       " 999.986114052572]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[150,\n",
       " 105.34801721547944,\n",
       " 126.13471917628125,\n",
       " 116.45794862200977,\n",
       " 120.96274606972909,\n",
       " 118.86564116059868,\n",
       " 119.84189984026605,\n",
       " 119.38742488614261,\n",
       " 119.59899502291616,\n",
       " 119.50050320781361,\n",
       " 119.54635359313434,\n",
       " 119.52500879150305,\n",
       " 119.53494516153384,\n",
       " 119.53031930255781,\n",
       " 119.53247255390217,\n",
       " 119.53146994657168,\n",
       " 119.53193647656232,\n",
       " 119.53171908350993,\n",
       " 119.53182007507831,\n",
       " 119.53177285001942,\n",
       " 119.53179462379771]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent For Higher Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to use many parameters in our model? Gradient descent actually scales to as many variables as you want. Each parameter value will need its own update rule, and it closemly matches the update rule for $a_1$:\n",
    "\n",
    "\n",
    "$\\displaystyle  a_0 := a_0 - \\alpha \\dfrac{d}{da_0} MSE \\\\\n",
    " a_1 := a_1 - \\alpha \\dfrac{d}{da_1} MSE \\\\ \n",
    " a_2 := a_2 - \\alpha \\dfrac{d}{da_2} MSE \\\\ \n",
    " \\vdots\\\\\n",
    " a_n := a_n - \\alpha \\dfrac{d}{da_n} MSE \\\\\n",
    " $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the derivative for the MSE with respect to the intercept value $(a_0)$, the derivatives for other parameters are identical:\n",
    "\n",
    "$\\displaystyle  \\dfrac{d}{da_1} MSE = \\frac{2}{n} \\sum_{i=1}^{n} x_1^{(i)}\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\\\  \n",
    "\\displaystyle \\dfrac{d}{da_2} MSE = \\dfrac{2}{n} \\sum_{i=1}^{n} x_2^{(i)}\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\\\\n",
    " \\vdots\\\\\n",
    "\\displaystyle \\dfrac{d}{da_n} MSE = \\dfrac{2}{n} \\sum_{i=1}^{n} x_n^{(i)}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)  \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mission, we explored how to find a linear regression model using the gradient descent algorithm. The main challenges with gradient descent include:\n",
    "\n",
    "* Choosing good initial parameter values\n",
    "* Choosing a good learning rate (falls under the domain of hyperparameter optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
